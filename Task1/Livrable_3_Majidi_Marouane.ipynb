{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7443e847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class GridWorld:\n",
    "    ACTIONS = {\n",
    "        0: (-1, 0),  # UP\n",
    "        1: (1, 0),   # DOWN\n",
    "        2: (0, -1),  # LEFT\n",
    "        3: (0, 1)    # RIGHT\n",
    "    }\n",
    "    ACTION_NAMES = {0: \"UP\", 1: \"DOWN\", 2: \"LEFT\", 3: \"RIGHT\"}\n",
    "    ACTION_ARROWS = {0: \"↑\", 1: \"↓\", 2: \"←\", 3: \"→\"}\n",
    "\n",
    "    def __init__(self, height=5, width=5, start=(0, 0), goal=(4, 4), step_penalty=-1, goal_reward=5, gamma=0.8):\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.start = start\n",
    "        self.goal = goal\n",
    "        self.step_penalty = step_penalty\n",
    "        self.goal_reward = goal_reward\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def state_index(self, row, col):\n",
    "        return row * self.width + col\n",
    "\n",
    "    def state_coords(self, state):\n",
    "        return divmod(state, self.width)\n",
    "\n",
    "    def random_policy(self):\n",
    "        policy = {}\n",
    "        for s in range(self.width * self.height):\n",
    "            policy[s] = {a: 1.0 / len(self.ACTIONS) for a in self.ACTIONS}\n",
    "        return policy\n",
    "\n",
    "    def reward(self, state):\n",
    "        row, col = self.state_coords(state)\n",
    "        return self.goal_reward if (row, col) == self.goal else self.step_penalty\n",
    "\n",
    "    def step(self, state, action):\n",
    "        row, col = self.state_coords(state)\n",
    "        dr, dc = self.ACTIONS[action]\n",
    "\n",
    "        nr, nc = row + dr, col + dc\n",
    "        nr = max(0, min(self.height - 1, nr))\n",
    "        nc = max(0, min(self.width - 1, nc))\n",
    "\n",
    "        next_state = self.state_index(nr, nc)\n",
    "        reward = self.reward(next_state)\n",
    "        done = (nr, nc) == self.goal\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def deterministic_policy(self, stochastic_policy):\n",
    "        policy = {}\n",
    "        rewards = {}\n",
    "\n",
    "        for s in range(self.width * self.height):\n",
    "            max_prob = max(stochastic_policy[s].values())\n",
    "            best_actions = [a for a, p in stochastic_policy[s].items() if abs(p - max_prob) < 1e-8]\n",
    "            chosen_action = random.choice(best_actions)\n",
    "            policy[s] = {a: 1.0 if a == chosen_action else 0.0 for a in self.ACTIONS}\n",
    "            rewards[s] = self.reward(s)\n",
    "\n",
    "        return policy, rewards\n",
    "\n",
    "    def run_episode(self, policy, max_steps=50):\n",
    "        state = self.state_index(*self.start)\n",
    "        trajectory = [state]\n",
    "        rewards = [0]   # discounted returns\n",
    "        total_reward = 0\n",
    "        discount = 1.0\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "            # Pick greedy action\n",
    "            action = max(policy[state], key=policy[state].get)\n",
    "            next_state, r, done = self.step(state, action)\n",
    "\n",
    "            # Apply discount factor\n",
    "            total_reward += discount * r\n",
    "            discount *= self.gamma\n",
    "\n",
    "            trajectory.append(next_state)\n",
    "            rewards.append(total_reward)\n",
    "\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        return trajectory, rewards\n",
    "\n",
    "\n",
    "    def visualise_episode(self, policy, cols=5):\n",
    "        \"\"\"Static plots of agent moving step by step in a row/column grid layout.\"\"\"\n",
    "        trajectory, rewards = self.run_episode(policy)\n",
    "        n_steps = len(trajectory)\n",
    "\n",
    "        rows = (n_steps + cols - 1) // cols  # ceil division\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(3*cols, 3*rows))\n",
    "\n",
    "        # Flatten axes for easy iteration\n",
    "        axes = axes.flatten() if isinstance(axes, (list, np.ndarray)) else [axes]\n",
    "\n",
    "        for i in range(n_steps):\n",
    "            ax = axes[i]\n",
    "            # Draw grid\n",
    "            for x in range(self.width + 1):\n",
    "                ax.plot([x, x], [0, self.height], color=\"black\")\n",
    "            for y in range(self.height + 1):\n",
    "                ax.plot([0, self.width], [y, y], color=\"black\")\n",
    "\n",
    "            # Mark start & goal\n",
    "            sr, sc = self.start\n",
    "            gr, gc = self.goal\n",
    "            ax.text(sc + 0.5, self.height - sr - 0.5, \"S\", ha=\"center\", va=\"center\",\n",
    "                    fontsize=14, color=\"blue\", fontweight=\"bold\")\n",
    "            ax.text(gc + 0.5, self.height - gr - 0.5, \"T\", ha=\"center\", va=\"center\",\n",
    "                    fontsize=14, color=\"green\", fontweight=\"bold\")\n",
    "\n",
    "            # Mark agent\n",
    "            row, col = self.state_coords(trajectory[i])\n",
    "            ax.plot(col + 0.5, self.height - row - 0.5, \"ro\", markersize=12)\n",
    "\n",
    "            # Title with step and cumulative reward\n",
    "            ax.set_title(f\"Step {i}\\nReward={rewards[i]:.1f}\")\n",
    "\n",
    "            ax.set_xlim(0, self.width)\n",
    "            ax.set_ylim(0, self.height)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.set_aspect(\"equal\")\n",
    "\n",
    "        # Hide unused subplots\n",
    "        for j in range(n_steps, len(axes)):\n",
    "            axes[j].axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def visualise_policy(self, policy, title=\"Policy\"):\n",
    "        \"\"\"Show the deterministic/stochastic policy with arrows in the grid.\"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(self.width, self.height))\n",
    "\n",
    "        # Draw grid\n",
    "        for x in range(self.width + 1):\n",
    "            ax.plot([x, x], [0, self.height], color=\"black\")\n",
    "        for y in range(self.height + 1):\n",
    "            ax.plot([0, self.width], [y, y], color=\"black\")\n",
    "\n",
    "        for row in range(self.height):\n",
    "            for col in range(self.width):\n",
    "                s = self.state_index(row, col)\n",
    "                x, y = col + 0.5, self.height - row - 0.5\n",
    "\n",
    "                if (row, col) == self.goal:\n",
    "                    ax.text(x, y, \"T\", ha=\"center\", va=\"center\", fontsize=16,\n",
    "                            color=\"green\", fontweight=\"bold\")\n",
    "                else:\n",
    "                    # Best actions\n",
    "                    max_prob = max(policy[s].values())\n",
    "                    best_actions = [a for a, p in policy[s].items() if abs(p - max_prob) < 1e-8]\n",
    "\n",
    "                    arrows = \"\".join([self.ACTION_ARROWS[a] for a in best_actions])\n",
    "                    ax.text(x, y, arrows, ha=\"center\", va=\"center\", fontsize=14, color=\"red\")\n",
    "\n",
    "        ax.set_xlim(0, self.width)\n",
    "        ax.set_ylim(0, self.height)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_aspect(\"equal\")\n",
    "        ax.set_title(title, fontsize=14)\n",
    "        plt.show()\n",
    "\n",
    "    def value_iteration(self, theta=1e-6):\n",
    "        \"\"\"\n",
    "        Perform value iteration for this GridWorld.\n",
    "        Returns:\n",
    "            V (dict): state -> value\n",
    "            policy (dict): state -> {action: prob}\n",
    "        \"\"\"\n",
    "        n_states = self.width * self.height\n",
    "        V = {s: 0.0 for s in range(n_states)}  # init values\n",
    "        policy = self.random_policy()          # init policy\n",
    "\n",
    "        while True:\n",
    "            delta = 0.0\n",
    "            new_V = V.copy()\n",
    "\n",
    "            for s in range(n_states):\n",
    "                row, col = self.state_coords(s)\n",
    "                if (row, col) == self.goal:  # terminal state\n",
    "                    continue\n",
    "\n",
    "                q_values = {}\n",
    "                for a in self.ACTIONS:\n",
    "                    s2, r, _ = self.step(s, a)\n",
    "                    q_values[a] = r + self.gamma * V[s2]\n",
    "\n",
    "                best_q = max(q_values.values())\n",
    "                best_actions = [a for a, q in q_values.items() if abs(q - best_q) < 1e-8]\n",
    "\n",
    "                # update value\n",
    "                new_V[s] = best_q\n",
    "\n",
    "                for a in self.ACTIONS:\n",
    "                    policy[s][a] = 1.0 if a in best_actions else 0.0\n",
    "\n",
    "                delta = max(delta, abs(new_V[s] - V[s]))\n",
    "\n",
    "            V = new_V\n",
    "            if delta < theta:\n",
    "                break\n",
    "\n",
    "        return V, policy\n",
    "    \n",
    "    def visualize_values_matrix(self, V):\n",
    "        height, width = self.height, self.width\n",
    "        V_list = [V[s] for s in range(height * width)]\n",
    "        V_matrix = np.array(V_list).reshape((height, width))\n",
    "        return V_matrix\n",
    "    \n",
    "    def policy_iteration(self, theta=1e-6, max_iterations=100):\n",
    "\n",
    "        n_states = self.width * self.height\n",
    "        # initi policy\n",
    "        policy = self.random_policy()\n",
    "        V = {s: 0.0 for s in range(n_states)}\n",
    "\n",
    "        for it in range(max_iterations):\n",
    "            # Policy Evaluation\n",
    "            while True:\n",
    "                delta = 0.0\n",
    "                new_V = V.copy()\n",
    "\n",
    "                for s in range(n_states):\n",
    "                    row, col = self.state_coords(s)\n",
    "                    if (row, col) == self.goal:\n",
    "                        continue\n",
    "\n",
    "                    v = 0.0\n",
    "                    for a, prob in policy[s].items():\n",
    "                        s2, r, _ = self.step(s, a)\n",
    "                        v += prob * (r + self.gamma * V[s2])\n",
    "\n",
    "                    new_V[s] = v\n",
    "                    delta = max(delta, abs(new_V[s] - V[s]))\n",
    "\n",
    "                V = new_V\n",
    "                if delta < theta:\n",
    "                    break\n",
    "\n",
    "            # Policy Improvement\n",
    "            policy_stable = True\n",
    "            for s in range(n_states):\n",
    "                row, col = self.state_coords(s)\n",
    "                if (row, col) == self.goal:\n",
    "                    continue\n",
    "\n",
    "                # old best action\n",
    "                old_best_action = max(policy[s], key=policy[s].get)\n",
    "\n",
    "                # compute action values\n",
    "                q_values = {}\n",
    "                for a in self.ACTIONS:\n",
    "                    s2, r, _ = self.step(s, a)\n",
    "                    q_values[a] = r + self.gamma * V[s2]\n",
    "\n",
    "                best_q = max(q_values.values())\n",
    "                best_actions = [a for a, q in q_values.items() if abs(q - best_q) < 1e-8]\n",
    "\n",
    "                # update policy greedily\n",
    "                for a in self.ACTIONS:\n",
    "                    policy[s][a] = 1.0 if a in best_actions else 0.0\n",
    "\n",
    "                # check stability\n",
    "                if old_best_action not in best_actions:\n",
    "                    policy_stable = False\n",
    "\n",
    "            if policy_stable:\n",
    "                break\n",
    "\n",
    "        return V, policy\n",
    "\n",
    "    def q_learning(self,alpha):\n",
    "        #initial alpha\n",
    "        epsilon = np.max(0.05,0.2)\n",
    "        alpha = V = {s: 0.0 for s,a in range(n_states)}\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
